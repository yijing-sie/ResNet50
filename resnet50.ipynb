{"cells":[{"cell_type":"markdown","id":"1d7e8e44-ba3a-4ff4-b60c-8319db76a669","metadata":{"id":"1d7e8e44-ba3a-4ff4-b60c-8319db76a669"},"source":["# Read me\n","1. To successfully run this code, one has to put all the folders downloaded from kaggle under the folder where this code is being run.\n","2. Ceate a folder called **path** where all the pth files will be stored.\n","3. Run cells one after another to train and test the model\n","\n","## Model Description\n","* This is a resnet50 model with normalized data. The architecture of the model is the same as [torchvision.models.resnet50](https://pytorch.org/vision/stable/models.html#torchvision.models.resnet50). All the hyperparameters are tuned based on this website [ResNet 50 v1.5](https://ngc.nvidia.com/catalog/resources/nvidia:resnet_50_v1_5_for_pytorch) and the posts on piazza.\n","\n","* Details about the model structure is written in a text cell above the corresponding code cell."]},{"cell_type":"code","execution_count":null,"id":"764e3441-0246-4c8b-b9eb-dbe7e5b5286c","metadata":{"id":"764e3441-0246-4c8b-b9eb-dbe7e5b5286c","outputId":"dd6495c5-e2fd-4b46-f1c3-b3a97627d6dc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda device\n"]}],"source":["import numpy as np\n","import torch\n","from torch import nn, optim, unsqueeze`a\n","from tqdm import tqdm\n","import torch.nn.functional as F\n","from torchvision import transforms, datasets\n","import torchvision\n","from PIL import Image\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print('Using {} device'.format(device))\n","batchsize = 80"]},{"cell_type":"markdown","id":"b641c581-f768-4fdb-8988-b0e430146706","metadata":{"id":"b641c581-f768-4fdb-8988-b0e430146706"},"source":["Normalization idea came from this post[@1097](https://piazza.com/class/knsmz2b3z131mn?cid=1097)\n","and other transformation ideas came from the observation of the trainning data"]},{"cell_type":"code","execution_count":null,"id":"032fcd41-e4ea-4174-91dc-f6c5d031e97e","metadata":{"id":"032fcd41-e4ea-4174-91dc-f6c5d031e97e"},"outputs":[],"source":["#%%transform\n","transform_train = transforms.Compose([\n","    transforms.ColorJitter(brightness=.5, hue=.3),\n","    transforms.RandomRotation(degrees=45),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"]},{"cell_type":"code","execution_count":null,"id":"47f0fe64-8fa1-4875-ad5d-a380aef3e867","metadata":{"id":"47f0fe64-8fa1-4875-ad5d-a380aef3e867"},"outputs":[],"source":["#%%Loading data\n","train_dataset = ImageFolder(root = 'train_data/', transform= transform_train)\n","val_dataset = ImageFolder(root = 'val_data/', transform= transform)\n","train_dataloader = DataLoader(train_dataset,\n","                              batch_size = batchsize,\n","                              num_workers=4,\n","                              shuffle=True,\n","                              pin_memory = True)\n","val_dataloader = DataLoader(val_dataset,\n","                            batch_size = batchsize,\n","                            num_workers=4,\n","                            shuffle=False,\n","                            pin_memory = True)"]},{"cell_type":"markdown","id":"d2f9ab76-c59b-4df8-9937-c389b3bbb3f1","metadata":{"id":"d2f9ab76-c59b-4df8-9937-c389b3bbb3f1"},"source":["* I built **my_resnet50** by printing out \"torchvision.models.resnet50\" to see \n","how the different layers are constructed and their specifications.\n","\n","\n","* However, given that the size of the input image is only 64x64, I modified the kernel size and the stride in the first convolution to be 1 such that no downsampling happens here; furthermore I also got rid of the first maxpooling layer to prevent downsampling before entering the residual blocks.\n","\n","\n","* The embedding size is according to one of the comments posted under this [post](https://piazza.com/class/knsmz2b3z131mn?cid=831) @831_f36 : \"In general, small embedding sizes (512 - 1024) are enough to get an A cutoff.\"\n","\n","* Droupout is added because I faced some severe early trainning overfitting"]},{"cell_type":"code","execution_count":null,"id":"cf81ce42-6f59-43c3-bf9d-336a52eec3c1","metadata":{"id":"cf81ce42-6f59-43c3-bf9d-336a52eec3c1"},"outputs":[],"source":["#%%building resnet50 from scratch\n","class Bottleneck(nn.Module):\n","    def __init__(self, in_channel, exp, first_block=False, stride = 1):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=64*(2**exp), kernel_size=1, stride=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(num_features=64*(2**exp))\n","        self.conv2 = nn.Conv2d(in_channels=64*(2**exp), out_channels=64*(2**exp), kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(num_features=64*(2**exp))\n","        self.conv3 = nn.Conv2d(in_channels=64*(2**exp), out_channels=256*(2**exp), kernel_size=1, stride=1,bias=False)                \n","        self.bn3 = nn.BatchNorm2d(num_features=256*(2**exp))\n","        self.relu = nn.ReLU(inplace=True)\n","        if first_block:\n","            self.downsample = nn.Sequential(\n","                nn.Conv2d(in_channels=in_channel, out_channels=256*(2**exp), kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(num_features=256*(2**exp))\n","                )\n","        else:\n","            self.downsample = nn.Identity()\n","    def forward(self, X):\n","        out = self.conv1(X)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","        out = self.relu(out + self.downsample(X))\n","        return out"]},{"cell_type":"code","execution_count":null,"id":"c4bb00d0-4f46-4466-9d5a-5ecd77733b0f","metadata":{"id":"c4bb00d0-4f46-4466-9d5a-5ecd77733b0f"},"outputs":[],"source":["#%%\n","class my_resnet50(nn.Module):\n","    def __init__(self, num_classes = 4000):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=1, stride=1, padding=0, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.relu = nn.ReLU(inplace=True)      \n","        self.layer1 = nn.Sequential(\n","            Bottleneck(in_channel =  64, exp = 0, first_block=True),\n","            Bottleneck(in_channel = 256, exp = 0, first_block=False),\n","            Bottleneck(in_channel = 256, exp = 0, first_block=False)\n","            )\n","        self.layer2 = nn.Sequential(\n","            Bottleneck(in_channel = 256, exp = 1, first_block=True, stride=2),\n","            Bottleneck(in_channel = 512, exp = 1, first_block=False),\n","            Bottleneck(in_channel = 512, exp = 1, first_block=False),\n","            Bottleneck(in_channel = 512, exp = 1, first_block=False),\n","            )\n","        self.layer3 = nn.Sequential(\n","            Bottleneck(in_channel = 512, exp = 2, first_block=True, stride=2),\n","            Bottleneck(in_channel = 1024, exp = 2, first_block=False),\n","            Bottleneck(in_channel = 1024, exp = 2, first_block=False),\n","            Bottleneck(in_channel = 1024, exp = 2, first_block=False),\n","            Bottleneck(in_channel = 1024, exp = 2, first_block=False),\n","            Bottleneck(in_channel = 1024, exp = 2, first_block=False),\n","            )\n","        self.layer4 = nn.Sequential(\n","            Bottleneck(in_channel =  1024, exp = 3, first_block=True, stride=2),\n","            Bottleneck(in_channel = 2048, exp = 3, first_block=False),\n","            Bottleneck(in_channel = 2048, exp = 3, first_block=False)\n","            )\n","        self.avgpool= nn.AdaptiveAvgPool2d(output_size=(1, 1))\n","        self.flatten = nn.Flatten()\n","        self.fc = nn.Linear(in_features=2048, out_features=4000, bias=True)\n","        self.embedding = nn.Linear(in_features=2048, out_features=512, bias=True)#for valification\n","        self.dropout = nn.Dropout()\n","    def forward(self, X, return_embedding=False):\n","        out = self.conv1(X)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = self.avgpool(out)\n","        out = self.flatten(out)\n","        out = self.dropout(out)\n","        if return_embedding:\n","            embedding_out = self.relu(self.embedding(out))\n","            output = self.fc(out)\n","            return embedding_out,output\n","        else:\n","            output = self.fc(out)\n","            return output"]},{"cell_type":"code","execution_count":null,"id":"ef0c4a54-ea5d-44f2-8197-aa9a25076f28","metadata":{"id":"ef0c4a54-ea5d-44f2-8197-aa9a25076f28"},"outputs":[],"source":["config={\n","      \"epoch\": 50,\n","      \"lr\": 0.128,\n","      \"momentum\": 0.875, \n","      \"weight_decay\": 5e-5\n","}"]},{"cell_type":"code","execution_count":null,"id":"-hCrFqnroH-t","metadata":{"id":"-hCrFqnroH-t"},"outputs":[],"source":["#%% Set model parameters\n","#https://ngc.nvidia.com/catalog/resources/nvidia:resnet_50_v1_5_for_pytorch\n","model = my_resnet50()\n","model.to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=config['lr'], momentum=config['momentum'], weight_decay=config['weight_decay'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)\n"]},{"cell_type":"code","execution_count":null,"id":"325f29bd-ab49-4186-8e39-83001e809ea7","metadata":{"id":"325f29bd-ab49-4186-8e39-83001e809ea7"},"outputs":[],"source":["#%%train\n","def training(dataloader, model, criterion, optimizer):\n","    sum_loss, accuracy = 0.0, 0.0\n","    total_correct = 0\n","    total = 0\n","    n_batches = len(dataloader) #number of batches\n","    #train mode\n","    model.train()\n","    for (X, y) in dataloader:\n","        #sending data to device\n","        X, y = X.float().to(device), y.to(device)\n","        #Forward\n","        optimizer.zero_grad()\n","        prediction = model(X)\n","        loss = criterion(prediction, y)\n","\n","        # Backpropagation        \n","        loss.backward()\n","        optimizer.step()\n","        sum_loss += loss \n","        y_hat = prediction.argmax(1)\n","        total_correct += torch.sum(y_hat == y)\n","        total += y.size(0)\n","    mean_loss = sum_loss.item() / n_batches\n","    accuracy = total_correct.item()/ total\n","    return mean_loss, accuracy"]},{"cell_type":"code","execution_count":null,"id":"394bf8ac-3cec-4186-9f10-a12962b6c6ed","metadata":{"id":"394bf8ac-3cec-4186-9f10-a12962b6c6ed"},"outputs":[],"source":["#%%validating\n","def testing(dataloader, model):\n","    accuracy = 0.0\n","    total_correct = 0\n","    total = 0\n","    n_batches = len(dataloader) #number of batches\n","    model.eval() \n","    with torch.no_grad():\n","        for (X, y) in dataloader:\n","            #sending data to device\n","            X, y = X.float().to(device), y.to(device)\n","            #Forward\n","            prediction = model(X)\n","            #calculating loss\n","            y_hat = prediction.argmax(1)            \n","            total_correct += torch.sum(y_hat == y)\n","            total += y.size(0)\n","    accuracy = total_correct.item()/ total\n","    return accuracy"]},{"cell_type":"code","execution_count":null,"id":"8e093122-6a70-432c-b226-67d6b34d65b1","metadata":{"id":"8e093122-6a70-432c-b226-67d6b34d65b1"},"outputs":[],"source":["#%%running model\n","print('Running model')\n","for e in tqdm(range(epoch)):\n","    #trainning\n","    train_loss, train_accuracy = training(train_dataloader, model, criterion, optimizer)\n","    #Saving & Loading a General Checkpoint for Inference and/or Resuming Training\n","    val_accuracy = testing(val_dataloader, model)\n","    # Note that step should be called after validate()\n","    if e > 9:\n","        scheduler.step(val_accuracy) \n","        torch.save({\n","        'epoch': e,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'val_accuracy': val_accuracy}, 'path/'+str(e)+'.pth')\n","    print({\"Epoch\": str(e), \n","          \"Train Loss\": train_loss,\n","          \"Train Accuracy\": train_accuracy,\n","          \"Validation Accuracy\": val_accuracy}) \n","print('Done.')\n"]},{"cell_type":"markdown","id":"ugEkjCYcpAiA","metadata":{"id":"ugEkjCYcpAiA"},"source":["Load the model with highest validation accuracy. In my case, it is 43 with validation accuracy  =  0.8504"]},{"cell_type":"code","execution_count":null,"id":"NszsZwaYro9j","metadata":{"id":"NszsZwaYro9j"},"outputs":[],"source":["config2={\n","      \"epoch\": 50,\n","      \"lr\": 0.001,\n","      \"momentum\": 0.875, \n","      \"weight_decay\": 5e-5\n","}"]},{"cell_type":"code","execution_count":null,"id":"6adafc14-1b8d-4142-905a-e477fb6e31dd","metadata":{"id":"6adafc14-1b8d-4142-905a-e477fb6e31dd"},"outputs":[],"source":["#%% Set model parameters\n","#https://ngc.nvidia.com/catalog/resources/nvidia:resnet_50_v1_5_for_pytorch\n","# T_max value should be num_epochs\n","checkpoint = torch.load('path/43.pth')\n","model = my_resnet50()\n","model.load_state_dict(checkpoint['model_state_dict'])\n","model.to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=config['lr'], momentum=config['momentum'], weight_decay=config['weight_decay'])\n","#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=7)"]},{"cell_type":"code","execution_count":null,"id":"-ERUFQ_-J2aA","metadata":{"id":"-ERUFQ_-J2aA"},"outputs":[],"source":["#%%running model\n","print('Running model')\n","epochs = checkpoint['epoch']\n","for e in tqdm(range(epochs+1,epochs+epoch)):\n","    #trainning\n","    train_loss, train_accuracy = training(train_dataloader, model, criterion, optimizer)\n","    #Saving & Loading a General Checkpoint for Inference and/or Resuming Training\n","    val_accuracy = testing(val_dataloader, model)\n","    # Note that step should be called after validate()\n","    if e > epochs+5:\n","        #scheduler.step(val_accuracy) \n","        scheduler.step() \n","        torch.save({\n","        'epoch': e,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'val_accuracy': val_accuracy}, 'path/'+str(e)+'.pth')\n","    print({\"Epoch\": str(e), \n","          \"Train Loss\": train_loss,\n","          \"Train Accuracy\": train_accuracy,\n","          \"Validation Accuracy\": val_accuracy}) \n","print('Done.')"]},{"cell_type":"markdown","id":"zojUuRU1sI-Y","metadata":{"id":"zojUuRU1sI-Y"},"source":["The final model is the one with the highest validation accuracy (at least 0.87 )"]},{"cell_type":"markdown","id":"6ad34c24-0781-4613-b799-49bf91dbb6e3","metadata":{"id":"6ad34c24-0781-4613-b799-49bf91dbb6e3"},"source":["# Prediction\n","for prediction, please manually load the model with the highest validation accuracy."]},{"cell_type":"code","execution_count":null,"id":"Z9rEa4eDtIVR","metadata":{"id":"Z9rEa4eDtIVR"},"outputs":[],"source":["checkpoint = torch.load('path/93.pth') #load the model with the highest validation accuracy.\n","model = my_resnet50()\n","model.load_state_dict(checkpoint['model_state_dict'])\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"id":"4a12f922-229a-45f8-8d54-83e4d0806472","metadata":{"id":"4a12f922-229a-45f8-8d54-83e4d0806472"},"outputs":[],"source":["import glob\n","file_list = list(glob.glob(\"test_data/*\"))"]},{"cell_type":"code","execution_count":null,"id":"13a05aca-094b-43a0-acba-b8d3e2762087","metadata":{"id":"13a05aca-094b-43a0-acba-b8d3e2762087"},"outputs":[],"source":["class ImageDataset(torch.utils.data.Dataset):\n","    def __init__(self, file_list):\n","        self.file_list = file_list\n","\n","    def __len__(self):\n","        return len(self.file_list)\n","\n","    def __getitem__(self, index):\n","        img = Image.open(self.file_list[index])\n","        img = transform(img)\n","        return img"]},{"cell_type":"code","execution_count":null,"id":"df62b78f-30bf-4ad3-a392-ce30e1e0167a","metadata":{"id":"df62b78f-30bf-4ad3-a392-ce30e1e0167a"},"outputs":[],"source":["test_dataset = ImageDataset(file_list)\n","test_dataloader = DataLoader(test_dataset,\n","                            batch_size = 1,\n","                            num_workers=4,\n","                            shuffle=False,\n","                            pin_memory = True)"]},{"cell_type":"code","execution_count":null,"id":"b0ee3a8a-6cdf-458f-9a5e-ba1c440d4896","metadata":{"id":"b0ee3a8a-6cdf-458f-9a5e-ba1c440d4896"},"outputs":[],"source":["class_to_idx = train_dataset.class_to_idx"]},{"cell_type":"code","execution_count":null,"id":"c93c42e4-6c4a-4a1b-8f8b-c8f68d625fac","metadata":{"id":"c93c42e4-6c4a-4a1b-8f8b-c8f68d625fac","outputId":"ee245f74-e360-4929-9daa-e9eec21df235"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████████████████████████████████| 8000/8000 [00:51<00:00, 155.16it/s]\n"]}],"source":["#%%testing\n","prediction = []\n","model.eval()\n","with torch.no_grad():\n","    for X in tqdm(test_dataloader):\n","        X = X.float().to(device)\n","        #Predicting\n","        prediction += [model(X).argmax(1).item()]          "]},{"cell_type":"code","execution_count":null,"id":"f0383af9-3310-4721-94b4-f30344cf915b","metadata":{"id":"f0383af9-3310-4721-94b4-f30344cf915b"},"outputs":[],"source":["def get_label(p):\n","    for i, (key, value) in enumerate(class_to_idx.items()):  # for name, age in dictionary.iteritems():  (for Python 2.x)\n","        if value == p:\n","            return key\n","    print(i,\"has no matched key\")"]},{"cell_type":"code","execution_count":null,"id":"b3551537-3369-46bd-a96b-1819d7987b7f","metadata":{"id":"b3551537-3369-46bd-a96b-1819d7987b7f"},"outputs":[],"source":["test_prediction = list(map(get_label, prediction))"]},{"cell_type":"code","execution_count":null,"id":"0b0ff870-2cfe-4960-8f4d-c9d170a682b4","metadata":{"id":"0b0ff870-2cfe-4960-8f4d-c9d170a682b4"},"outputs":[],"source":["#%%save prediction to csv\n","import pandas as pd\n","submit = pd.read_csv(\"classification_sample_submission.csv\")"]},{"cell_type":"code","execution_count":null,"id":"a4b9d58a-7148-4468-a9b4-e7058b1228ee","metadata":{"id":"a4b9d58a-7148-4468-a9b4-e7058b1228ee"},"outputs":[],"source":["submit[\"label\"] = test_prediction"]},{"cell_type":"code","execution_count":null,"id":"f3598a60-713c-4b70-ad1c-3ee7a847764e","metadata":{"id":"f3598a60-713c-4b70-ad1c-3ee7a847764e"},"outputs":[],"source":["submit.to_csv('Hw2classification.csv', index= False)"]},{"cell_type":"markdown","id":"1cb39e46-60c5-4f40-80a6-d761361bfc66","metadata":{"id":"1cb39e46-60c5-4f40-80a6-d761361bfc66"},"source":["# Face Verification\n","* For verification task, please reuse the model that is used for prediciton\n","* please put the file downloaded from kaggle under the same folder where the code is run"]},{"cell_type":"markdown","id":"683_pX1jxFb4","metadata":{"id":"683_pX1jxFb4"},"source":["Run the following cell for face verification task"]},{"cell_type":"code","execution_count":null,"id":"MuPe8T5jxfBm","metadata":{"id":"MuPe8T5jxfBm"},"outputs":[],"source":["val_pair = []\n","with open('verification_pairs_test.txt') as f:\n","    lines = f.readlines()\n","for line in lines:\n","    val_pair += [line.split()]"]},{"cell_type":"code","execution_count":null,"id":"n1qFYA0OwjU8","metadata":{"id":"n1qFYA0OwjU8"},"outputs":[],"source":["model.eval()\n","similarity = []\n","labels = []\n","compute_sim = nn.CosineSimilarity(dim=0)\n","with torch.no_grad():\n","    for i in tqdm(val_pair):\n","        im1 = Image.open('verification_data/'+i[0])\n","        im1 = transforms.ToTensor()(im1)\n","        im1 = im1.unsqueeze(0)\n","        im1 = im1.float().to(device)\n","        im2 = Image.open('verification_data/'+i[1])\n","        im2 = transform(im2)\n","        im2 = im2.unsqueeze(0)\n","        im2 = im2.float().to(device)\n","        eb1 = model(im1).squeeze(0)\n","        eb2 = model(im2).squeeze(0)\n","        similarity.append(compute_sim(eb1, eb2).item())"]},{"cell_type":"code","execution_count":null,"id":"luJQ9F5G0UqM","metadata":{"id":"luJQ9F5G0UqM"},"outputs":[],"source":["#%%save prediction to csv\n","import pandas as pd\n","submit_verification = pd.read_csv(\"verification_solution_sample.csv\")"]},{"cell_type":"code","execution_count":null,"id":"iz3PnPP_4bBi","metadata":{"id":"iz3PnPP_4bBi"},"outputs":[],"source":["submit_verification['Category'] = similarity"]},{"cell_type":"code","execution_count":null,"id":"cr7lgq5B4b0J","metadata":{"id":"cr7lgq5B4b0J"},"outputs":[],"source":["submit_verification.to_csv('Hw2classification.csv', index= False)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"hw2p2.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}
